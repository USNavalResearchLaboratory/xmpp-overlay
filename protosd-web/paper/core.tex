In the experiments discussed in this \doctype, we expand on previous experiments, described in Section \ref{sec:indi}, by applying JmDNS and INDI to a full emulated scenario that is designed to empirically measure the differences in message overhead and latency for a realistic scenario.  Therefore, for the experimental evaluation in comparing the performance of INDI and JmDNS, described in \ref{sec:methodology} and \ref{sec:results}, we used the CORE Emulation framework \cite{ahrenholz2008core}. 

The Common Open Research Emulator (CORE)  is a network emulator that runs real computer networks in real time.  This is quite distinct from  simulation (e.g. ns-2 \cite{NS2_webpage}), which builds a model of a real network and then triggers events at discrete time intervals in order to simulate a actual application's use of that network.    Although simulators can be used to provide targeted empirical evaluations, generally most experiments are limited in scope and provide very focused and controlled experiments which make a number of functional assumptions compared to what a real application might do.   Therefore, the deployment of such applications in real-world environments can often be problematic and lead to a number of unmeasured issues and bugs that are caused by both the application's software implementation (code issues, multi-threading, socket performance, etc) and underlying network problems that can significantly affect performance.  Consequently, and timely due to the growth and maturity of lightweight virtual machine implementations, we have chosen to use the CORE emulation environment for our INDI experiments, in order to to gather real-world deployment experience for moving to production quality more expeditiously. 

Briefly, CORE has two components:  a CORE services (daemon), which manages the emulation sessions and interfaces with the underlying Kernal virtualization; and the graphical user interface, which is a drag and drop GUI that can be used to orchestrate the creation of the nodes. These two components communicate using an asynchronous, sockets-based CORE API, which is written in Python. 

The Core services daemon, on Ubuntu 11.04, interfaces with Linux network namespaces virtualization to build its virtual nodes. Linux namespaces builds up from chroot to implement complete virtual systems, adding resource management and isolation mechanisms to the existing process management infrastructure.  The result is that each node is a lightweight virtual system with full resource isolation and resource control for an application; that is, each node essentially provides a lightweight virtual machine, which can all access a common host filesystem (although permissions can be set so that some directories are private on a per-container basis).  Linux network namespaces (netns) is part of the mainline 2.6.27$+$ Linux kernel.
 
From a network perspective,  each namespace has its own loopback device and process space. Virtual or real devices can be added to each network namespace, and you can assign IP addresses to these devices and use them as a network node.  The virtual networks are built using the Linux Ethernet bridging and for the Wireless implementation we use here, employ the use of OSPF routing protocol configurations  using the Quagga \cite{quagga} open source routing suite.  The MDR router type in CORE builds OSPF MANET Designated Routers (MDR) configurations which requires a modified version of Quagga \cite{ospf-mdr}.   Quagga OSPFv3 is such an implementation of  OSPF MDR \cite{rfc5614},  OSPF Database Exchange Optimization \cite{rfc5243}, and  OSPFv3 Address Families \cite{rfc5838}, for efficient routing in mobile ad hoc networks (MANETs).

